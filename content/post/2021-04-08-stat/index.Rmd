---
title: '[통계용어]데이터분석A-Z'
author: iris
date: '2021-04-08'
slug: stat
categories:
  - R
tags:
  - statistics
---

### -다중공선성

수리적으로, 어떤 독립 변수가 다른 독립 변수들과 완벽한 **선형독립**이 아닌 경우를 뜻한다.    
즉, 일부 독립변수가 다른 독립변수와의 상관성이 높아 분석에 있어서 부정적인 영향을 미치는 현상을 의미한다.  
다중공선성이 생기면 해당 변수들 각각의 설명력이 약해진다. 그 이유는 다중공선성으로 인하여 X1에서 설명해야할 부분을 X2가 가져가기 때문에, p-value가 유의수준 미만으로 떨어지지 못하고, 그로 인해 귀무가설을 기각하지 못하는 문제가 발생한다.  

이러한 문제를 해결하기 위해 R에서는 car::vif() 함수를 이용하여 해결하고자 한다.  
VIF(Variance Inflation Factor : 분산팽창요인)는 종속변수를 제외하고 2개 이상의 독립변수 조합 간의 회귀분석을 따로 실시한다. 실시 결과 설명력이 높은 경우 다중공선성 문제가 발생되는 것으로 판단한다.



### -train:test = 70:30 split

70을 학습용 데이터셋으로 활용하고, 30은 테스트용 데이터셋으로 사용한다.  
그리고 70으로 선정된 학습용 데이터셋은 내부에서 다시 train set과 validation set으로 나뉘게 된다.  

[Example]  
Training = 교과서  
Validation = 실전 모의고사  
Test = 실전 시험  


### -클래스 불균형 해소 방안

+ 언더 샘플링 
  + 단점 : 잠재적으로 정보의 가치가 높은 데이터도 버려져서 유용한 데이터가 사라지는 위험이 있다.


+ 오버 샘플링
  + 단점 : 동일한 데이터를 똑같이 복사해서 수만 늘리기 때문에 과대적합의 위험이 생긴다.

+ SMOTE
  + 특징 : 소수 클래스에서 각각의 샘플들의 knn을 찾고, 그 이웃들 사이에 선을 그어 무작위 점을 생성한다.
  + 이렇게 했을 경우엔 샘플들 사이의 특성들을 반영한 데이터가 생성되기 때문에 과적합을 완전히 방지하지는 못하지만 어느정도 해소할 수 있다. 


+ 알고리즘 내 하이퍼 파라미터 사용
  + scale_pos_weight의 값을 늘려주며 클래스 불균형 해소 가능 (default = 0) 



### -XG boost 
Gradient Boosting의 단점을 보완하기 위해 나온 기법으로, GBM의 단점에 대해 우선 알아보자.

+ 느리다
+ 과적합의 이슈가 존재

이러한 단점을 보완한 XGB 는 아래와 같은 특징을 가진다.

+ 빠르다 (GBM보다 빠른 것)
+ 과적합 방지가 가능한 규제가 포함되어 있다.
+ 분류와 회귀 둘다 가능하다.
+ early stopping 기능을 제공한다

### -하이퍼 파라미터
+ nrounds : iteration을 몇번 진행할 지 

+ 1 epoch : 전체 데이터셋에 대해 한번의 학습과정이 완료되었다고 이해  
  epoch 값이 너무 작으면 과소적합, epoch 값이 너무 크면 과대적합 문제가 발생할 수 있다.

+ nfold : training/validation으로 나누는 비율을 몇으로 할 지 정하는 값으로 보편적으로 5fold를 사용한다.

+ map_depth : 트리의 깊이 (0 ~ 무한대)  
  의사결정 나무의 깊이의 한도. 깊이가 깊어질 수록 복잡한 모델이 생성되고, 이는 과적합 문제를 발생시킬 수 있다.
  
+ eta : 학습률  (0 ~ 1)  
  학습 단계별 가중치를 얼마나 적용할 지 결정하는 숫자이고, 가중치이므로 0~1 사이의 값을 가진다.
  
+ gamma : min split loss (0 ~ 무한대)  
  information gain 에 페널티를 부여하는 숫자이므로, 이 값이 높아질수록 Gain값이 낮아져, 더이상 가지를 만들려하지 않게되어 보수적인 모델이 될 수 있다.

+ subsample : subset 비율 (0 ~ 1)  
  training set에서 subset을 만들지 전부를 사용할지를 정하는 파라미터 - 과적합 방지 (0 ~ 1)

+ colsample_bytree : 컬럼의 샘플링 비율 (0 ~ 1)  
  트리를 생성할 때, 변수를 샘플링하는 비율
  
+ binary:logistic : 이진 분류 모델 생성의 경우


### -Light GBM

+ 특징  
  + xg-boost보다, 짧은 학습시간과 낮은 메모리 사용량
  + 작은 데이터셋인 경우 과적합 문제가 발생할 수 있다.
  + 일반 GBM 계열과 달리 리프중심 트리분할 방식을 적용한다.
  + 비대칭 규칙 트리를 생성한다. 하지만 이렇게 최대 손실값을 가지는 리프 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할 수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화할 수 있다.



### -grid search 

일반화 성능을 최대로 높여주는 모델의 하이퍼파라미터 값을 찾는 방법  

관심있는 하이퍼파라미터들을 대상으로 가능한 모든 조합을 시도하여 최적 값을 찾는 방법


### -정밀도(precision)
TP / (TP + FP)  
=> 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 오답을 정답으로 예측 수[제 1종 오류])


### -재현율(recall)  
TP / (TP + FN)  
=> 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 정답을 오답으로 예측 수[제 2종 오류])



### -ROC curve
curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가.  
즉 recall이 크고 fall-out이 작은 모형이 좋은 모형인 것.


### -AUC
ROC curve는 그래프이기 때문에 명확한 수치로 비교하기 어렵다. 따라서 그래프 아래의 면적값인 AUC로 표현한다.  
최대값은 1이며 fall-out에 비해 recall값이 클수록 1에 가까운 값을 가진다.
