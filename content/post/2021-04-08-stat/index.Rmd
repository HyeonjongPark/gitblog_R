---
title: '[통계기술]'
author: iris
date: '2021-04-08'
slug: stat
categories:
  - R
tags:
  - statistics
---

### -다중공선성

수리적으로, 어떤 독립 변수가 다른 독립 변수들과 완벽한 **선형독립**이 아닌 경우를 뜻한다.    
즉, 일부 독립변수가 다른 독립변수와의 상관성이 높아 분석에 있어서 부정적인 영향을 미치는 현상을 의미한다.  
다중공선성이 생기면 해당 변수들 각각의 설명력이 약해진다. 그 이유는 다중공선성으로 인하여 X1에서 설명해야할 부분을 X2가 가져가기 때문에, p-value가 유의수준 미만으로 떨어지지 못하고, 그로 인해 귀무가설을 기각하지 못하는 문제가 발생한다.  

이러한 문제를 해결하기 위해 R에서는 car::vif() 함수를 이용하여 해결하고자 한다.  
VIF(Variance Inflation Factor : 분산팽창요인)는 종속변수를 제외하고 2개 이상의 독립변수 조합 간의 회귀분석을 따로 실시한다. 실시 결과 설명력이 높은 경우 다중공선성 문제가 발생되는 것으로 판단한다.



### -train:test = 70:30 split

70을 학습용 데이터셋으로 활용하고, 30은 테스트용 데이터셋으로 사용한다.  
그리고 70으로 선정된 학습용 데이터셋은 내부에서 다시 train set과 validation set으로 나뉘게 된다.  

[Example]  
Training = 교과서
Validation = 실전 모의고사
Test = 실전 시험




### -분류모델 

### -Light GBM

### -XG boost 
Gradient Boosting의 단점을 보완하기 위해 나온 기법으로, GBM의 단점에 대해 우선 알아보자.

+ 느리다
+ 과적합의 이슈가 존재

이러한 단점을 보완한 XGB 는 아래와 같은 특징을 가진다.

+ 빠르다 (GBM보다 빠른 것)
+ 과적합 방지가 가능한 규제가 포함되어 있다.
+ 분류와 회귀 둘다 가능하다.
+ early stopping 기능을 제공한다

### -하이퍼 파라미터
+ nrounds : iteration을 몇번 진행할 지 

+ nfold : training/validation으로 나누는 비율을 몇으로 할 지 정하는 값으로 보편적으로 5fold를 사용한다.

+ map_depth : 트리의 깊이 (0 ~ 무한대)  
  의사결정 나무의 깊이의 한도. 깊이가 깊어질 수록 복잡한 모델이 생성되고, 이는 과적합 문제를 발생시킬 수 있다.
  
+ eta : 학습률  (0 ~ 1)  
  학습 단계별 가중치를 얼마나 적용할 지 결정하는 숫자이고, 가중치이므로 0~1 사이의 값을 가진다.
  
+ gamma : min split loss (0 ~ 무한대)  
  information gain 에 페널티를 부여하는 숫자이므로, 이 값이 높아질수록 Gain값이 낮아져, 더이상 가지를 만들려하지 않게되어 보수적인 모델이 될 수 있다.

+ subsample : subset 비율 (0 ~ 1)  
  training set에서 subset을 만들지 전부를 사용할지를 정하는 파라미터 - 과적합 방지 (0 ~ 1)

+ colsample_bytree : 컬럼의 샘플링 비율 (0 ~ 1)  
  트리를 생성할 때, 변수를 샘플링하는 비율
  
+ binary:logistic : 이진 분류 모델 생성의 경우

### -grid search 

일반화 성능을 최대로 높여주는 모델의 하이퍼파라미터 값을 찾는 방법  

관심있는 하이퍼파라미터들을 대상으로 가능한 모든 조합을 시도하여 최적 값을 찾는 방법


### -정밀도(precision)
TP / (TP + FP)  
=> 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 오답을 정답으로 예측 수[제 1종 오류])


### -재현율(recall)  
TP / (TP + FN)  
=> 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 정답을 오답으로 예측 수[제 2종 오류])



### -ROC curve
curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가.  
즉 recall이 크고 fall-out이 작은 모형이 좋은 모형인 것.


### -AUC
ROC curve는 그래프이기 때문에 명확한 수치로 비교하기 어렵다. 따라서 그래프 아래의 면적값인 AUC로 표현한다.  
최대값은 1이며 fall-out에 비해 recall값이 클수록 1에 가까운 값을 가진다.
