---
title: '[통계기술]'
author: iris
date: '2021-04-08'
slug: stat
categories:
  - R
tags:
  - statistics
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="다중공선성" class="section level3">
<h3>-다중공선성</h3>
<p>수리적으로, 어떤 독립 변수가 다른 독립 변수들과 완벽한 <strong>선형독립</strong>이 아닌 경우를 뜻한다.<br />
즉, 일부 독립변수가 다른 독립변수와의 상관성이 높아 분석에 있어서 부정적인 영향을 미치는 현상을 의미한다.<br />
다중공선성이 생기면 해당 변수들 각각의 설명력이 약해진다. 그 이유는 다중공선성으로 인하여 X1에서 설명해야할 부분을 X2가 가져가기 때문에, p-value가 유의수준 미만으로 떨어지지 못하고, 그로 인해 귀무가설을 기각하지 못하는 문제가 발생한다.</p>
<p>이러한 문제를 해결하기 위해 R에서는 car::vif() 함수를 이용하여 해결하고자 한다.<br />
VIF(Variance Inflation Factor : 분산팽창요인)는 종속변수를 제외하고 2개 이상의 독립변수 조합 간의 회귀분석을 따로 실시한다. 실시 결과 설명력이 높은 경우 다중공선성 문제가 발생되는 것으로 판단한다.</p>
</div>
<div id="traintest-7030-split" class="section level3">
<h3>-train:test = 70:30 split</h3>
<p>70을 학습용 데이터셋으로 활용하고, 30은 테스트용 데이터셋으로 사용한다.<br />
그리고 70으로 선정된 학습용 데이터셋은 내부에서 다시 train set과 validation set으로 나뉘게 된다.</p>
<p>[Example]<br />
Training = 교과서
Validation = 실전 모의고사
Test = 실전 시험</p>
</div>
<div id="분류모델" class="section level3">
<h3>-분류모델</h3>
</div>
<div id="light-gbm" class="section level3">
<h3>-Light GBM</h3>
</div>
<div id="xg-boost" class="section level3">
<h3>-XG boost</h3>
<p>Gradient Boosting의 단점을 보완하기 위해 나온 기법으로, GBM의 단점에 대해 우선 알아보자.</p>
<ul>
<li>느리다</li>
<li>과적합의 이슈가 존재</li>
</ul>
<p>이러한 단점을 보완한 XGB 는 아래와 같은 특징을 가진다.</p>
<ul>
<li>빠르다 (GBM보다 빠른 것)</li>
<li>과적합 방지가 가능한 규제가 포함되어 있다.</li>
<li>분류와 회귀 둘다 가능하다.</li>
<li>early stopping 기능을 제공한다</li>
</ul>
</div>
<div id="하이퍼-파라미터" class="section level3">
<h3>-하이퍼 파라미터</h3>
<ul>
<li><p>nrounds : iteration을 몇번 진행할 지</p></li>
<li><p>nfold : training/validation으로 나누는 비율을 몇으로 할 지 정하는 값으로 보편적으로 5fold를 사용한다.</p></li>
<li><p>map_depth : 트리의 깊이 (0 ~ 무한대)<br />
의사결정 나무의 깊이의 한도. 깊이가 깊어질 수록 복잡한 모델이 생성되고, 이는 과적합 문제를 발생시킬 수 있다.</p></li>
<li><p>eta : 학습률 (0 ~ 1)<br />
학습 단계별 가중치를 얼마나 적용할 지 결정하는 숫자이고, 가중치이므로 0~1 사이의 값을 가진다.</p></li>
<li><p>gamma : min split loss (0 ~ 무한대)<br />
information gain 에 페널티를 부여하는 숫자이므로, 이 값이 높아질수록 Gain값이 낮아져, 더이상 가지를 만들려하지 않게되어 보수적인 모델이 될 수 있다.</p></li>
<li><p>subsample : subset 비율 (0 ~ 1)<br />
training set에서 subset을 만들지 전부를 사용할지를 정하는 파라미터 - 과적합 방지 (0 ~ 1)</p></li>
<li><p>colsample_bytree : 컬럼의 샘플링 비율 (0 ~ 1)<br />
트리를 생성할 때, 변수를 샘플링하는 비율</p></li>
<li><p>binary:logistic : 이진 분류 모델 생성의 경우</p></li>
</ul>
</div>
<div id="grid-search" class="section level3">
<h3>-grid search</h3>
<p>일반화 성능을 최대로 높여주는 모델의 하이퍼파라미터 값을 찾는 방법</p>
<p>관심있는 하이퍼파라미터들을 대상으로 가능한 모든 조합을 시도하여 최적 값을 찾는 방법</p>
</div>
<div id="정밀도precision" class="section level3">
<h3>-정밀도(precision)</h3>
<p>TP / (TP + FP)<br />
=&gt; 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 오답을 정답으로 예측 수[제 1종 오류])</p>
</div>
<div id="재현율recall" class="section level3">
<h3>-재현율(recall)</h3>
<p>TP / (TP + FN)<br />
=&gt; 정답을 정답으로 예측 수 / (정답을 정답으로 예측 수 + 정답을 오답으로 예측 수[제 2종 오류])</p>
</div>
<div id="roc-curve" class="section level3">
<h3>-ROC curve</h3>
<p>curve가 왼쪽 위 모서리에 가까울수록 모델의 성능이 좋다고 평가.<br />
즉 recall이 크고 fall-out이 작은 모형이 좋은 모형인 것.</p>
</div>
<div id="auc" class="section level3">
<h3>-AUC</h3>
<p>ROC curve는 그래프이기 때문에 명확한 수치로 비교하기 어렵다. 따라서 그래프 아래의 면적값인 AUC로 표현한다.<br />
최대값은 1이며 fall-out에 비해 recall값이 클수록 1에 가까운 값을 가진다.</p>
</div>
